{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import btgsolutions_dataservices as btg\n",
    "\n",
    "load_dotenv()\n",
    "BTG_API_KEY = os.getenv(\"BTG_API_KEY\")\n",
    "bulk = btg.BulkData(api_key=BTG_API_KEY)\n",
    "\n",
    "def _filter_session(df: pd.DataFrame, date: str, time_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col], utc=True)\n",
    "    start = pd.to_datetime(f\"{date} 13:00:00\").tz_localize(\"UTC\")\n",
    "    end = pd.to_datetime(f\"{date} 20:50:00\").tz_localize(\"UTC\")\n",
    "    return df[df[time_col].between(start, end)]\n",
    "\n",
    "\n",
    "def get_trade_data(ticker: str, year: str, month: str, day: str) -> pd.DataFrame:\n",
    "    date = f\"{year}-{month}-{day}\"\n",
    "    cols = [\"symbol\", \"rpt_seq\", \"network_received_time\", \"md_entry_px\",\n",
    "            \"md_entry_size\", \"aggressor\"]\n",
    "    df = bulk.get_data(ticker, date, data_type=\"trades\")[cols]\n",
    "    df = _filter_session(df, date, \"network_received_time\")\n",
    "    df = df.drop_duplicates([\"md_entry_px\", \"md_entry_size\", \"aggressor\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_book_data(ticker: str, year: str, month: str, day: str) -> pd.DataFrame:\n",
    "    date = f\"{year}-{month}-{day}\"\n",
    "    cols = [\"symbol\", \"rpt_seq\", \"network_received_time\", \"msg_type\", \"en_b_px_1\", \"en_s_px_1\"]\n",
    "    df = bulk.get_data(ticker, date, data_type=\"books\")[cols]\n",
    "    df = df[df[\"msg_type\"] == \"INCREMENTAL\"]\n",
    "    df = _filter_session(df, date, \"network_received_time\")\n",
    "    df = df[(df[\"en_b_px_1\"] != 0) & (df[\"en_s_px_1\"] != 0)]\n",
    "    df = df.drop_duplicates([\"en_b_px_1\", \"en_s_px_1\"])\n",
    "    df = df.drop(columns=[\"msg_type\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_book_trade(book_df: pd.DataFrame, trade_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    trade_df = trade_df.rename(columns={\"symbol\": \"symbol_trade\",\n",
    "                                        \"network_received_time\": \"network_received_time_trade\"})\n",
    "    book_df = book_df.rename(columns={\"symbol\": \"symbol_book\",\n",
    "                                      \"network_received_time\": \"network_received_time_book\"})\n",
    "\n",
    "    df = pd.merge(trade_df, book_df, on=\"rpt_seq\", how=\"outer\")\n",
    "\n",
    "    df[\"symbol\"] = df.pop(\"symbol_trade\").combine_first(df.pop(\"symbol_book\"))\n",
    "    df[\"network_received_time\"] = df.pop(\"network_received_time_trade\").combine_first(\n",
    "        df.pop(\"network_received_time_book\")\n",
    "    )\n",
    "    df[\"network_received_time\"] = pd.to_datetime(df[\"network_received_time\"], utc=True)\n",
    "\n",
    "    df = df.sort_values([\"network_received_time\", \"rpt_seq\"]).reset_index(drop=True)\n",
    "\n",
    "    has_trade = df[\"md_entry_px\"].notna() if \"md_entry_px\" in df else pd.Series(False, index=df.index)\n",
    "    has_book = pd.Series(False, index=df.index)\n",
    "    if \"en_b_px_1\" in df:\n",
    "        has_book = has_book | df[\"en_b_px_1\"].notna()\n",
    "    if \"en_s_px_1\" in df:\n",
    "        has_book = has_book | df[\"en_s_px_1\"].notna()\n",
    "\n",
    "    df[\"event_type\"] = np.select(\n",
    "        [has_trade, has_book],\n",
    "        [\"trade\", \"book\"],\n",
    "        default=\"unknown\"\n",
    "    )\n",
    "\n",
    "    first_book_idx = df.index[df[\"event_type\"] == \"book\"]\n",
    "    if not first_book_idx.empty:\n",
    "        df = df.loc[first_book_idx[0]:].reset_index(drop=True)\n",
    "\n",
    "    if \"en_b_px_1\" in df:\n",
    "        df[\"en_b_px_1\"] = df[\"en_b_px_1\"].ffill()\n",
    "    if \"en_s_px_1\" in df:\n",
    "        df[\"en_s_px_1\"] = df[\"en_s_px_1\"].ffill()\n",
    "\n",
    "    numeric_cols = [\n",
    "        \"md_entry_px\", \"md_entry_size\", \"aggressor\",\n",
    "        \"en_b_px_1\", \"en_s_px_1\"\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    ordered_cols = [\n",
    "        \"symbol\",\n",
    "        \"rpt_seq\",\n",
    "        \"network_received_time\",\n",
    "        \"event_type\",\n",
    "        \"md_entry_px\",\n",
    "        \"md_entry_size\",\n",
    "        \"aggressor\",\n",
    "        \"en_s_px_1\",\n",
    "        \"en_b_px_1\",\n",
    "    ]\n",
    "    ordered_cols = [c for c in ordered_cols if c in df.columns]\n",
    "    remaining = [c for c in df.columns if c not in ordered_cols]\n",
    "\n",
    "    df = df[ordered_cols + remaining]\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_df(df: pd.DataFrame, ticker: str, year: str, month: str, day: str, out_dir: str = \"train_data\", index: int = None) -> None:\n",
    "    Path(out_dir).mkdir(exist_ok=True)\n",
    "    if index is not None:\n",
    "        out = Path(out_dir) / f\"{index}_{ticker}_{day}{month}{year}.csv\"\n",
    "    else:\n",
    "        out = Path(out_dir) / f\"{ticker}_{day}{month}{year}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"âœ” salvo {out} ({len(df)} linhas)\")\n",
    "\n",
    "\n",
    "def prepare_data(ticker: str, year: str, month: str, day: str, out_dir: str, index: int = None) -> None:\n",
    "    try:\n",
    "        book_df = get_book_data(ticker, year, month, day)\n",
    "        trade_df = get_trade_data(ticker, year, month, day)\n",
    "        df = merge_book_trade(book_df, trade_df)\n",
    "        save_df(df, ticker, year, month, day, out_dir, index)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro {ticker} {year}-{month}-{day}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get train data from PETR4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"PETR4\"\n",
    "year = \"2025\"\n",
    "month = \"03\"\n",
    "# day = \"05\"\n",
    "# prepare_data(ticker, year, month, day, out_dir=\"train_data\")\n",
    "\n",
    "for day in range(1, 32):\n",
    "    print(f\"Processing {ticker} for {year}-{month}-{day}\")\n",
    "    try:\n",
    "        prepare_data(ticker, year, month, str(day).zfill(2), out_dir=\"train_data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker} on {year}-{month}-{day}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "auth_url = \"https://dataservices.btgpactualsolutions.com/api/v2/authenticate\"\n",
    "auth_headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "auth_body = {\n",
    "    \"api_key\": BTG_API_KEY,\n",
    "    \"client_id\": \"string\"\n",
    "}\n",
    "\n",
    "liquidity_url = \"https://dataservices.btgpactualsolutions.com/api/v1/marketdata/analytics/top-n-liquidity?n=\"\n",
    "liquidity_headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer \"\n",
    "}\n",
    "\n",
    "def get_auth_token():\n",
    "    response = requests.post(auth_url, json=auth_body, headers=auth_headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"AccessToken\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to authenticate: {response.status_code} {response.text}\")\n",
    "    \n",
    "def get_top_n_liquidiyty_tickers(token, n=15):\n",
    "    url = liquidity_url + str(n)\n",
    "    liquidity_headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    response = requests.get(url, headers=liquidity_headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json().get(\"result\")\n",
    "        return data\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch liquidity data: {response.status_code} {response.text}\")\n",
    "\n",
    "def filter_top_liquidity_tickers(data):\n",
    "    if len(data) <= 15:\n",
    "        return [item['ticker'] for item in data]\n",
    "    top_5 = data[:5]\n",
    "    bottom_5 = data[-5:]\n",
    "    middle_5 = np.random.choice(data[5:-5], 5, replace=False)\n",
    "    selected = []\n",
    "    selected.extend(top_5)\n",
    "    selected.extend(middle_5.tolist())\n",
    "    selected.extend(bottom_5)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = get_auth_token()\n",
    "all_tickers = get_top_n_liquidiyty_tickers(token, 200)\n",
    "all_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tickers = [\n",
    " 'VALE3',\n",
    " 'PETR4',\n",
    " 'ITUB4',\n",
    " 'BBAS3',\n",
    " 'BBDC4',\n",
    " 'ETHE11',\n",
    " 'RECV3',\n",
    " 'BIAU39',\n",
    " 'ENEV3',\n",
    " 'MDIA3',\n",
    " 'CMIG4',\n",
    " 'AMBP3',\n",
    " 'TUPY3',\n",
    " 'XPML11',\n",
    " 'P2LT34',\n",
    "]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2025\"\n",
    "month = \"04\"\n",
    "day = \"11\"\n",
    "\n",
    "for day in range(7, 8):\n",
    "    count = 0\n",
    "    for index, ticker in enumerate(tickers):\n",
    "        print(f\"Processing {ticker} for {year}-{month}-{day}\")\n",
    "        try:\n",
    "            prepare_data(ticker, year, month, str(day).zfill(2), out_dir=\"test_data\", index=index+1)\n",
    "            count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker} on {year}-{month}-{day}: {e}\")\n",
    "            \n",
    "print(f\"{count} deram certo\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
